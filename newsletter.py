"""
Daily Telecom Newsletter Generator v5
======================================
- Personalised greeting per recipient
- Content in Portuguese
- Articles grouped by source type
- RSS + web scraping
"""

import os
import json
import datetime
import requests
import feedparser
import anthropic
import smtplib
import time
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from urllib.parse import urljoin, urlparse
from collections import defaultdict
import base64


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 0. SEEN URLs â€” persist via GitHub API
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SEEN_URLS_FILE = "seen_urls.txt"

def load_seen_urls(github_token: str, github_repo: str) -> tuple[set, str]:
    """Load seen URLs from GitHub repo. Returns (set of urls, file sha)."""
    if not github_token or not github_repo:
        return set(), ""
    try:
        url = f"https://api.github.com/repos/{github_repo}/contents/{SEEN_URLS_FILE}"
        r = requests.get(url, headers={"Authorization": f"token {github_token}"}, timeout=10)
        if r.status_code == 404:
            print("  seen_urls.txt nÃ£o existe ainda, a criar...")
            return set(), ""
        r.raise_for_status()
        data = r.json()
        sha = data.get("sha", "")
        decoded = base64.b64decode(data["content"]).decode("utf-8")
        urls = set(line.strip() for line in decoded.splitlines() if line.strip())
        print(f"  {len(urls)} URLs jÃ¡ vistos carregados")
        return urls, sha
    except Exception as e:
        print(f"[WARN] NÃ£o foi possÃ­vel carregar seen_urls.txt: {e}")
        return set(), ""


def save_seen_urls(github_token: str, github_repo: str, urls: set, sha: str):
    """Save seen URLs back to GitHub repo."""
    if not github_token or not github_repo:
        return
    try:
        content_str = "\n".join(sorted(urls))
        encoded = base64.b64encode(content_str.encode("utf-8")).decode("utf-8")
        api_url = f"https://api.github.com/repos/{github_repo}/contents/{SEEN_URLS_FILE}"
        payload = {
            "message": "chore: update seen_urls.txt",
            "content": encoded,
        }
        if sha:
            payload["sha"] = sha
        r = requests.put(api_url, json=payload,
                         headers={"Authorization": f"token {github_token}"}, timeout=15)
        r.raise_for_status()
        print(f"  seen_urls.txt actualizado ({len(urls)} URLs guardados)")
    except Exception as e:
        print(f"[WARN] NÃ£o foi possÃ­vel guardar seen_urls.txt: {e}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. LOAD CONFIG FROM GOOGLE SHEET
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_config_from_sheet(sheet_csv_urls: dict) -> dict:
    import csv, io

    def fetch_tab(url):
        r = requests.get(url, timeout=15)
        r.raise_for_status()
        reader = csv.DictReader(io.StringIO(r.text))
        return list(reader)

    feeds_rows      = fetch_tab(sheet_csv_urls["feeds"])
    keywords_rows   = fetch_tab(sheet_csv_urls["keywords"])
    recipients_rows = fetch_tab(sheet_csv_urls["recipients"])
    scrape_rows     = fetch_tab(sheet_csv_urls["scrape"])

    # feeds: name, url, active, group
    feeds = [
        {"url": r["url"], "group": r.get("Group") or r.get("group") or "Outras Fontes"}
        for r in feeds_rows
        if r.get("active","").lower() == "yes" and r.get("url")
    ]

    # keywords: keyword, active, groups (optional â€” comma-separated group names that restrict this keyword)
    keywords = []
    for r in keywords_rows:
        if r.get("active","").lower() != "yes" or not r.get("keyword"):
            continue
        groups_str = (r.get("Groups") or r.get("groups") or "").strip()
        restricted_groups = [g.strip() for g in groups_str.split(",") if g.strip()] if groups_str else []
        keywords.append({"keyword": r["keyword"], "restricted_groups": restricted_groups})

    # recipients: email, active, name
    recipients = [
        {"email": r["email"], "name": r.get("name", r["email"])}
        for r in recipients_rows
        if r.get("active","").lower() == "yes" and r.get("email")
    ]

    # scrape: name, url, selector, active, group
    scrape = [
        {"name": r["name"], "url": r["url"], "selector": r.get("selector","a"), "group": r.get("Group") or r.get("group") or "Outras Fontes"}
        for r in scrape_rows
        if r.get("active","").lower() == "yes" and r.get("url")
    ]

    return {"feeds": feeds, "keywords": keywords, "recipients": recipients, "scrape": scrape}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. FETCH & FILTER RSS FEEDS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def fetch_rss_articles(feeds: list, keywords: list, max_age_days: int = 1) -> list:
    cutoff = datetime.datetime.utcnow() - datetime.timedelta(days=max_age_days)
    articles = []

    for feed_cfg in feeds:
        url   = feed_cfg["url"]
        group = feed_cfg["group"]
        try:
            feed = feedparser.parse(url)
            source_name = feed.feed.get("title", url)

            for entry in feed.entries:
                published = None
                if hasattr(entry, "published_parsed") and entry.published_parsed:
                    published = datetime.datetime(*entry.published_parsed[:6])
                elif hasattr(entry, "updated_parsed") and entry.updated_parsed:
                    published = datetime.datetime(*entry.updated_parsed[:6])

                if published and published < cutoff:
                    continue

                title   = entry.get("title", "")
                summary = entry.get("summary", "") or entry.get("description", "")
                link    = entry.get("link", "")

                text = (title + " " + summary).lower()
                matched_keywords = [
                    kw["keyword"] for kw in keywords
                    if kw["keyword"].lower() in text
                    and (not kw["restricted_groups"] or group in kw["restricted_groups"])
                ]

                if matched_keywords:
                    articles.append({
                        "source":           source_name,
                        "group":            group,
                        "original_title":   title,
                        "original_summary": summary[:1000],
                        "link":             link,
                        "published":        published.strftime("%Y-%m-%d") if published else "unknown",
                        "matched_keywords": matched_keywords,
                        "type":             "rss",
                    })

        except Exception as e:
            print(f"[WARN] RSS failed for {url}: {e}")

    return articles


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. SCRAPE WEBSITES
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept-Language": "en-US,en;q=0.9",
}

def get_article_links(source_name: str, index_url: str, selector: str) -> list:
    try:
        from bs4 import BeautifulSoup
        r = requests.get(index_url, headers=HEADERS, timeout=20)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "html.parser")

        base_domain = f"{urlparse(index_url).scheme}://{urlparse(index_url).netloc}"
        links = []

        for sel in selector.split(","):
            sel = sel.strip()
            for tag in soup.select(sel):
                href = tag.get("href", "")
                if not href or href.startswith("#") or href.startswith("javascript"):
                    continue
                full_url = urljoin(base_domain, href)
                if urlparse(full_url).netloc == urlparse(index_url).netloc:
                    links.append((tag.get_text(strip=True), full_url))

        seen = set()
        unique = []
        for title, url in links:
            if url not in seen and len(title) > 5:
                seen.add(url)
                unique.append((title, url))

        print(f"  [{source_name}] {len(unique)} links encontrados")
        return unique[:30]

    except Exception as e:
        print(f"[WARN] Falhou ao obter links de {index_url}: {e}")
        return []


def scrape_article(url: str) -> str:
    try:
        from bs4 import BeautifulSoup
        r = requests.get(url, headers=HEADERS, timeout=20)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "html.parser")

        for tag in soup(["nav", "header", "footer", "script", "style", "aside", "form"]):
            tag.decompose()

        for selector in ["article", "main", ".content", ".article-body", ".entry-content", ".post-content", "#content"]:
            container = soup.select_one(selector)
            if container:
                text = container.get_text(separator=" ", strip=True)
                if len(text) > 200:
                    return text[:2000]

        paragraphs = soup.find_all("p")
        text = " ".join(p.get_text(strip=True) for p in paragraphs)
        return text[:2000]

    except Exception as e:
        return ""


def fetch_scraped_articles(scrape_config: list, keywords: list) -> list:
    try:
        from bs4 import BeautifulSoup
    except ImportError:
        print("[WARN] beautifulsoup4 nÃ£o instalado, a ignorar scraping")
        return []

    articles = []
    seen_urls = set()

    for cfg in scrape_config:
        source_name = cfg["name"]
        index_url   = cfg["url"]
        selector    = cfg["selector"]
        group       = cfg["group"]

        print(f"  A fazer scraping de {source_name}...")
        links = get_article_links(source_name, index_url, selector)

        for link_title, article_url in links:
            if article_url in seen_urls:
                continue
            seen_urls.add(article_url)

            text = scrape_article(article_url)
            if not text:
                continue

            full_text = (link_title + " " + text).lower()
            matched_keywords = [
                kw["keyword"] for kw in keywords
                if kw["keyword"].lower() in full_text
                and (not kw["restricted_groups"] or cfg["group"] in kw["restricted_groups"])
            ]

            if matched_keywords:
                articles.append({
                    "source":           source_name,
                    "group":            group,
                    "original_title":   link_title or article_url,
                    "original_summary": text[:1000],
                    "link":             article_url,
                    "published":        "unknown",
                    "matched_keywords": matched_keywords,
                    "type":             "scraped",
                })

            time.sleep(0.5)

    print(f"  Scraping concluÃ­do â€” {len(articles)} artigos encontrados")
    return articles


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. DEDUPLICATE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def deduplicate(articles: list) -> list:
    seen_urls   = set()
    seen_titles = set()
    unique = []

    for art in articles:
        url       = art["link"]
        title_key = art["original_title"].lower().strip()[:60]

        if url in seen_urls or title_key in seen_titles:
            continue

        seen_urls.add(url)
        seen_titles.add(title_key)
        unique.append(art)

    return unique


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. SUMMARISE WITH CLAUDE API (in Portuguese)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def summarise_articles(articles: list, api_key: str) -> list:
    if not articles:
        return []

    client = anthropic.Anthropic(api_key=api_key)
    summarised = []

    batch_size = 10
    for i in range(0, len(articles), batch_size):
        batch = articles[i:i+batch_size]

        articles_text = ""
        for j, art in enumerate(batch):
            articles_text += f"""
ARTIGO {j+1}:
Fonte: {art['source']}
TÃ­tulo original: {art['original_title']}
ConteÃºdo: {art['original_summary']}
---"""

        prompt = f"""Ã‰s o editor de uma newsletter profissional de telecomunicaÃ§Ãµes lida por especialistas em assuntos regulatÃ³rios da NOS (operadora portuguesa de telecomunicaÃ§Ãµes).

Para cada artigo abaixo, produz:
1. Um TÃTULO claro e profissional em portuguÃªs (mÃ¡ximo 12 palavras)
2. Um RESUMO em portuguÃªs de exatamente ~100 palavras que capture os factos principais, as implicaÃ§Ãµes regulatÃ³rias e a relevÃ¢ncia para a indÃºstria europeia de telecomunicaÃ§Ãµes.

Devolve a resposta como um array JSON, um objeto por artigo, com as chaves: "title" e "summary".
Devolve APENAS o array JSON, sem qualquer outro texto.

{articles_text}"""

        try:
            response = client.messages.create(
                model="claude-haiku-4-5-20251001",
                max_tokens=4000,
                messages=[{"role": "user", "content": prompt}]
            )

            raw = response.content[0].text.strip()
            # Strip markdown fences
            if "```" in raw:
                parts = raw.split("```")
                for part in parts:
                    part = part.strip()
                    if part.startswith("json"):
                        part = part[4:].strip()
                    if part.startswith("["):
                        raw = part
                        break
            raw = raw.strip()

            summaries = json.loads(raw)

            for j, art in enumerate(batch):
                s = summaries[j] if j < len(summaries) else {}
                art["title"]   = s.get("title", art["original_title"])
                art["summary"] = s.get("summary", "")
                # Validate we got a real summary, not empty
                if not art["summary"] or len(art["summary"]) < 20:
                    print(f"[WARN] Resumo vazio para artigo {j+1}, a tentar novamente...")
                    retry = client.messages.create(
                        model="claude-haiku-4-5-20251001",
                        max_tokens=500,
                        messages=[{"role": "user", "content": f'''Resume em portuguÃªs em ~100 palavras este artigo para uma newsletter de regulaÃ§Ã£o de telecomunicaÃ§Ãµes. Devolve apenas o resumo, sem mais texto.

TÃ­tulo: {art["original_title"]}
ConteÃºdo: {art["original_summary"]}'''}]
                    )
                    art["summary"] = retry.content[0].text.strip()
                summarised.append(art)

        except Exception as e:
            print(f"[WARN] Resumo falhou para o lote {i//batch_size + 1}: {e}")
            # Retry each article individually
            for art in batch:
                try:
                    retry = client.messages.create(
                        model="claude-haiku-4-5-20251001",
                        max_tokens=500,
                        messages=[{"role": "user", "content": f'''Resume em portuguÃªs em ~100 palavras este artigo para uma newsletter de regulaÃ§Ã£o de telecomunicaÃ§Ãµes. Devolve apenas o resumo, sem mais texto.

TÃ­tulo: {art["original_title"]}
ConteÃºdo: {art["original_summary"]}'''}]
                    )
                    art["title"]   = art["original_title"]
                    art["summary"] = retry.content[0].text.strip()
                except Exception as e2:
                    print(f"[WARN] Retry tambÃ©m falhou: {e2}")
                    art["title"]   = art["original_title"]
                    art["summary"] = "[Resumo nÃ£o disponÃ­vel]"
                summarised.append(art)

    return summarised


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. BUILD HTML EMAIL
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Group order and Portuguese labels
GROUP_ORDER = [
    "Reguladores Nacionais",
    "InstituiÃ§Ãµes Europeias",
    "PublicaÃ§Ãµes Nacionais",
    "PublicaÃ§Ãµes Especializadas",
    "Reguladores Europeus",
    "Outras Fontes",
]

def build_html_email(articles: list, date_str: str, recipient_name: str) -> str:
    # Group articles
    grouped = defaultdict(list)
    for art in articles:
        grouped[art.get("group", "Outras Fontes")].append(art)

    # Build sections in defined order
    sections = ""
    total = 0
    for group_name in GROUP_ORDER:
        group_articles = grouped.get(group_name, [])
        if not group_articles:
            continue

        total += len(group_articles)
        items = ""
        for art in group_articles:
            kw_tags = " ".join(
                f"<span style='background:#e8f4fd;color:#1a73e8;padding:2px 8px;border-radius:12px;font-size:11px;margin-right:4px;'>{kw}</span>"
                for kw in art.get("matched_keywords", [])
            )
            source_badge = ""
            if art.get("type") == "scraped":
                source_badge = "<span style='background:#fff3e0;color:#e65100;padding:2px 6px;border-radius:4px;font-size:10px;margin-left:6px;'>WEB</span>"

            pub = art.get("published","")
            pub_str = f" &nbsp;Â·&nbsp; {pub}" if pub and pub != "unknown" else ""

            items += f"""
            <div style="border-left:3px solid #1a73e8;padding:12px 16px;margin-bottom:20px;background:#fafafa;border-radius:0 6px 6px 0;">
                <p style="margin:0 0 4px 0;font-size:11px;color:#888;text-transform:uppercase;letter-spacing:0.5px;">
                    {art['source']}{source_badge}{pub_str}
                </p>
                <h3 style="margin:4px 0 8px 0;font-size:16px;color:#1a1a1a;">
                    <a href="{art['link']}" style="color:#1a1a1a;text-decoration:none;">{art['title']}</a>
                </h3>
                <p style="margin:0 0 10px 0;font-size:14px;color:#444;line-height:1.6;">{art['summary']}</p>
                <div style="margin-bottom:6px;">{kw_tags}</div>
                <a href="{art['link']}" style="font-size:12px;color:#1a73e8;">Ler artigo completo â†’</a>
            </div>"""

        sections += f"""
        <div style="margin-bottom:32px;">
            <h2 style="font-size:13px;font-weight:700;text-transform:uppercase;letter-spacing:1px;color:#1a73e8;border-bottom:2px solid #e8f0fe;padding-bottom:8px;margin-bottom:16px;">
                {group_name} <span style="font-weight:400;color:#999;">({len(group_articles)})</span>
            </h2>
            {items}
        </div>"""

    if not sections:
        sections = "<p style='color:#666;'>NÃ£o foram encontrados artigos relevantes hoje para as palavras-chave definidas.</p>"

    # First name only for greeting
    first_name = recipient_name.split()[0] if recipient_name else "colega"

    html = f"""<!DOCTYPE html>
<html>
<head><meta charset="UTF-8"></head>
<body style="font-family:Arial,sans-serif;max-width:700px;margin:0 auto;padding:20px;color:#1a1a1a;">

  <!-- Header -->
  <div style="background:linear-gradient(135deg,#1a73e8,#0d47a1);padding:24px 28px;border-radius:8px;margin-bottom:24px;">
    <h1 style="margin:0;color:white;font-size:22px;font-weight:700;">ğŸ“¡ InteligÃªncia RegulatÃ³ria Telecom</h1>
    <p style="margin:6px 0 0 0;color:#b3d4ff;font-size:13px;">{date_str} &nbsp;Â·&nbsp; {total} artigo{'s' if total!=1 else ''}</p>
  </div>

  <!-- Personalised greeting -->
  <p style="font-size:15px;color:#333;margin-bottom:24px;">OlÃ¡, {first_name}!</p>

  <!-- Grouped articles -->
  {sections}

  <!-- Footer -->
  <div style="border-top:1px solid #eee;margin-top:32px;padding-top:16px;font-size:11px;color:#999;">
    <p>Esta newsletter Ã© gerada automaticamente e enviada Ã s equipas de assuntos regulatÃ³rios da NOS.<br>
    Palavras-chave monitorizadas: Digital Â· 5G Â· Neutralidade da Rede Â· Internet Aberta Â· Cloud Â· ContribuiÃ§Ã£o Justa Â· InterligaÃ§Ã£o Â· AnÃ¡lise de Mercado Â· ServiÃ§o Universal Â· Espectro</p>
  </div>

</body>
</html>"""

    return html


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 7. SEND VIA GMAIL SMTP (one email per recipient)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 8. MAIN
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    today    = datetime.date.today()
    # Portuguese date format
    months_pt = ["janeiro","fevereiro","marÃ§o","abril","maio","junho",
                  "julho","agosto","setembro","outubro","novembro","dezembro"]
    weekdays_pt = ["segunda-feira","terÃ§a-feira","quarta-feira","quinta-feira",
                   "sexta-feira","sÃ¡bado","domingo"]
    date_str = f"{weekdays_pt[today.weekday()]}, {today.day} de {months_pt[today.month-1]} de {today.year}"

    CLAUDE_API_KEY       = os.environ["CLAUDE_API_KEY"]
    SHEET_URL_FEEDS      = os.environ["SHEET_URL_FEEDS"]
    SHEET_URL_KEYWORDS   = os.environ["SHEET_URL_KEYWORDS"]
    SHEET_URL_RECIPIENTS = os.environ["SHEET_URL_RECIPIENTS"]
    SHEET_URL_SCRAPE     = os.environ["SHEET_URL_SCRAPE"]
    GMAIL_ADDRESS        = os.environ["GMAIL_ADDRESS"]
    GMAIL_APP_PASSWORD   = os.environ["GMAIL_APP_PASSWORD"]
    GITHUB_TOKEN         = os.environ.get("GITHUB_TOKEN", "")
    GITHUB_REPO          = os.environ.get("GITHUB_REPOSITORY", "")

    print(f"[{date_str}] A iniciar geraÃ§Ã£o da newsletter...")

    # 1. Load config
    print("A carregar configuraÃ§Ã£o do Google Sheets...")
    config = load_config_from_sheet({
        "feeds":      SHEET_URL_FEEDS,
        "keywords":   SHEET_URL_KEYWORDS,
        "recipients": SHEET_URL_RECIPIENTS,
        "scrape":     SHEET_URL_SCRAPE,
    })
    print(f"  {len(config['feeds'])} feeds RSS | {len(config['scrape'])} sites scraping | {len(config['keywords'])} palavras-chave | {len(config['recipients'])} destinatÃ¡rios")

    # Load seen URLs
    print("A carregar URLs jÃ¡ vistos...")
    seen_urls, seen_sha = load_seen_urls(GITHUB_TOKEN, GITHUB_REPO)

    lookback = 3 if today.weekday() == 0 else 1

    # 2. Fetch RSS
    print("A obter feeds RSS...")
    rss_articles = fetch_rss_articles(config["feeds"], config["keywords"], max_age_days=lookback)
    print(f"  {len(rss_articles)} artigos RSS encontrados")

    # 3. Scrape websites
    print("A fazer scraping dos sites...")
    scraped_articles = fetch_scraped_articles(config["scrape"], config["keywords"])

    # 3b. Filter scraped articles against seen_urls
    new_scraped = [a for a in scraped_articles if a["link"] not in seen_urls]
    print(f"  {len(scraped_articles) - len(new_scraped)} artigos scraping jÃ¡ vistos ignorados, {len(new_scraped)} novos")
    scraped_articles = new_scraped

    # 4. Merge and deduplicate
    all_articles = deduplicate(rss_articles + scraped_articles)
    print(f"  Total apÃ³s deduplicaÃ§Ã£o: {len(all_articles)} artigos")

    # 5. Summarise
    if all_articles:
        print("A gerar resumos com Claude...")
        all_articles = summarise_articles(all_articles, CLAUDE_API_KEY)

    subject = f"ğŸ“¡ InteligÃªncia RegulatÃ³ria Telecom â€“ {today.day} de {months_pt[today.month-1]} de {today.year}"

    # 6. Send one personalised email per recipient
    print(f"A enviar emails para {len(config['recipients'])} destinatÃ¡rios...")
    for r in config["recipients"]:
        print(f"  DestinatÃ¡rio: {r['email']} ({r['name']})")
    with smtplib.SMTP_SSL("smtp.gmail.com", 465) as server:
        gmail_address      = GMAIL_ADDRESS.strip()
        gmail_app_password = GMAIL_APP_PASSWORD.strip().replace(" ", "")
        server.login(gmail_address, gmail_app_password)

        for recipient in config["recipients"]:
            first_name = recipient["name"].split()[0] if recipient["name"] else "colega"
            html_body  = build_html_email(all_articles, date_str, recipient["name"])

            msg = MIMEMultipart("alternative")
            msg["Subject"] = subject
            msg["From"]    = f"InteligÃªncia RegulatÃ³ria Telecom <{gmail_address}>"
            msg["To"]      = recipient["email"]
            msg.attach(MIMEText(html_body, "html"))

            server.sendmail(gmail_address, [recipient["email"]], msg.as_string())
            print(f"  [OK] Enviado para {recipient['email']} ({recipient['name']})")

    # Update seen_urls with all articles sent today
    new_url_set = seen_urls | {a["link"] for a in all_articles}
    # Keep only last 5000 URLs to avoid file growing forever
    if len(new_url_set) > 5000:
        new_url_set = set(sorted(new_url_set)[-5000:])
    print("A guardar URLs vistos...")
    save_seen_urls(GITHUB_TOKEN, GITHUB_REPO, new_url_set, seen_sha)

    print("ConcluÃ­do! âœ“")


if __name__ == "__main__":
    main()
